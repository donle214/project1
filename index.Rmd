---
title: 'Project 1: Wrangling, Exploration, Visualization'
author: "SDS322E"
date: ''
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F,
tidy=TRUE, tidy.opts=list(width.cutoff=60), R.options=list(max.print=100))
```

## Data Wrangling, Exploration, Visualization

### Don Le dhl595

#### Introduction 

```{R}

install.packages('KableExtra', dependencies=TRUE, repos='http://cran.rstudio.com/')

library(kableExtra)
library(tidyverse)
library(dplyr)

Hypertension_mortality_by_state <- read_csv("Hypertension_mortality_by_state.csv")
Heart_disease_mortality_by_state <- read_csv("Heart_disease_mortality_by_state.csv")
```


Paragraph or two introducing your datasets and variables, why they are interesting to you, etc.

*I chose two datasets that hold a special place in my heart. Not only do they relate to one of the current organizations that I work with (Hearts 4 the Homeless), but they also are known to be related clinically. Both data sets have prevalence mortality counts of hypertension and heart disease by year and state. In addition, they also have mortality counts per 100000 people. This is especially interesting to me because such data can be used to analyze whether more population dense states have differences in heart disease or hypertension mortality when compared to less population dense states. In addition, this analysis can be compared across years to see if there is an overall change in mortality rates for both of these diseases/conditions throughout the years. Statistically, I love working with a combination of categorical and numerical variables and a joined dataset of a public health statistic that holds a special place in my heart makes this project more meaningful!*

#### Tidying: Reshaping

If your datasets are tidy already, demonstrate that you can reshape data with pivot wider/longer here (e.g., untidy and then retidy). Alternatively, it may be easier to wait until the wrangling section so you can reshape your summary statistics. Note here if you are going to do this.

```{R}
Heart_disease_mortality_by_state <- Heart_disease_mortality_by_state %>% pivot_wider(names_from = STATE, values_from = RATE)
Hypertension_mortality_by_state <- Hypertension_mortality_by_state %>% pivot_wider(names_from = STATE, values_from = RATE)

Heart_disease_mortality_by_state <- Heart_disease_mortality_by_state %>% pivot_longer(4:54, names_to = "STATE", values_to = "Heart_Disease_Mortality_per_100000_People", values_drop_na = TRUE)
Hypertension_mortality_by_state <- Hypertension_mortality_by_state %>% pivot_longer(4:54, names_to = "STATE", values_to = "Hypertension_Mortality_per_100000_People", values_drop_na = TRUE)
```

*My datasets are already tidy. Here is a demonstration of reshaping using pivot_wider and pivot_longer. Please note that I also demonstrate this in the wrangling section.*

```{R}
#Cleaning up/removing unnecessary columns
Heart_disease_mortality_by_state <- Heart_disease_mortality_by_state %>% rename(Heart_Disease_Deaths = "DEATHS") %>% select(.,-URL)
Hypertension_mortality_by_state <- Hypertension_mortality_by_state %>% rename(Hypertension_Deaths = "DEATHS") %>% select(.,-URL)

```

*In this chunk, I remove unnecessary columns like ones with a website URL*
    
#### Joining/Merging

```{R}
#Inner Join by STATE and YEAR. The variable that I'm particularly interested in is STATE, but YEAR is also possible.
Mortality_joined <- inner_join(Heart_disease_mortality_by_state, Hypertension_mortality_by_state, by = c("STATE" = "STATE", "YEAR" = "YEAR"))

#Reorganize columns
Mortality_joined <- Mortality_joined[c("YEAR", "STATE", "Heart_Disease_Deaths", "Heart_Disease_Mortality_per_100000_People", "Hypertension_Deaths", "Hypertension_Mortality_per_100000_People")]
```

*I decided to choose an inner join because I wanted a joined dataset that included all of the ID intersections in the final join. While I was lucky that there weren't any NA's, they would have been removed due to this inner join which would make my end join much cleaner. This allows for more accurate plots.*

```{R}
#Number of unique ID's (STATE,YEAR) in each dataset
Heart_disease_mortality_by_state %>% n_distinct()
Hypertension_mortality_by_state %>% n_distinct()
```
*There are also 350 unique ID's (STATE, YEAR) in each dataset. This means that out of all the observations (350), none of the ID's were repeated and all of them were unique.*

```{R}
#Total number of observations in each dataset
Heart_disease_mortality_by_state %>% nrow()
Hypertension_mortality_by_state %>% nrow()
```
*There are 350 observations in both the Heart_disease_mortality_by_state and the Hypertension_mortality_by_state dataset.*

```{R}
#ID's (STATE,YEAR) that appear in one dataset but not the other
anti_join(Heart_disease_mortality_by_state, Hypertension_mortality_by_state) %>% n_distinct()
anti_join(Hypertension_mortality_by_state, Heart_disease_mortality_by_state) %>% n_distinct()


nrow(Mortality_joined)

350-350
```

*There were also 0 ID's that appeared in one data set and not the other. For both ways, this means that I was able to use an anti_join (Basically the opposite of inner join) which revealed such ID's. The data sets also have all 350 ID's (STATE, YEAR) in common. Compared to the size of the original datasets (350 each), the inner join allowed all the matched ID's to be combined so that the joined dataset ended up with 350 as well. In the joined dataset, there were 0 values that were dropped, which means that the joined dataset is representative of the data, and avoids any problems associated with dropped NA's. However, it is important to consider the problem of having such a small snapshot of data with no NA's being less representative of multiple decades than a large dataset spanning multiple decades with a few NA's.*

####  Wrangling

```{R - Core dplyr functions} 
#Core dplyr functions
Mortality_joined %>% filter(STATE == "AL")

Mortality_joined %>% select(STATE,`Heart_Disease_Deaths`)

Mortality_joined %>% arrange(desc('Heart_Disease_Mortality_per_100000_People'))

Mortality_joined %>% group_by(STATE)

Mortality_joined %>% mutate(Heart_Disease_Mortality_per_10000_People = (Heart_Disease_Mortality_per_100000_People)/10)

Mortality_joined %>% mutate(Hypertension_Mortality_per_10000_People = (Hypertension_Mortality_per_100000_People)/10)

Mortality_joined %>% summarize(mean(Heart_Disease_Deaths > 5000))

Mortality_joined %>% filter(str_detect(STATE,"^A"))

```
*For the procedure for my core dplyr functions, I will be piping my joined dataset into all of the functions. First, I used the filter function to filter only the AL state (Alabama). This is useful because it allows me to consolidate all AL's information throughout the years in one place. Then I used the select function to select only states and Heart Disease deaths. This is especially useful because I can remove all unnecessary information and focus on comparing two columns if I choose to in the future. Then, using the arrange function, I arranged the dataset based on Heart_Disease_Mortality_per_100000_People by decreasing number. This is very useful because now I can see which states and which years have the highest and lowest counts for any specific column I like. Then, using the group_by function, I grouped my data set by STATE which helped me consolidate any repeating state and year combinations. Luckily, I had already done this so this function would be useful if I ever used a different function that shuffled the states and I needed grouping again. Then, I used the mutate function to create a new column that shows both Heart Disease and Hypertension mortality per 10,000 people as opposed to 100,000. This was done very easily and can be modified to extract any number of statistics. Then, I used the summarize function to find the proportion of States with a Heart Disease death count of over 5000 people. This is very powerful and can be used to find if most or only a few states fall below or sit above a certain number. Lastly, I used filter to detect any states that start with A. This is especially useful when using text manipulation on certain strings. Using this function, I could see if states that start with A have a higher or lower death count for either of the diseases.*

```{R dplyr Summary Statistics Overall}
#5 Unique Dplyr Summary Statistics Functions
Mortality_joined %>% summarize_at(c("Heart_Disease_Deaths", "Heart_Disease_Mortality_per_100000_People", "Hypertension_Deaths", "Hypertension_Mortality_per_100000_People"), mean)
  
Mortality_joined %>% summarize_at(c("Heart_Disease_Deaths", "Heart_Disease_Mortality_per_100000_People", "Hypertension_Deaths", "Hypertension_Mortality_per_100000_People"), sd)

Mortality_joined %>% summarize_at(c("Heart_Disease_Deaths", "Heart_Disease_Mortality_per_100000_People", "Hypertension_Deaths", "Hypertension_Mortality_per_100000_People"), min)

Mortality_joined %>% summarize_at(c("Heart_Disease_Deaths", "Heart_Disease_Mortality_per_100000_People", "Hypertension_Deaths", "Hypertension_Mortality_per_100000_People"), max)

Mortality_joined %>% summarize_at(c("Heart_Disease_Deaths", "Heart_Disease_Mortality_per_100000_People", "Hypertension_Deaths", "Hypertension_Mortality_per_100000_People"), median)
```
*For the procedure of these summary statistics, I first piped my joined dataset into the summarize_at function which applies a given function to the selected columns in quotes. For the functions, I used, I decided to do a mean, standard deviation, minimum, maximum, and median, for each of the columns. These are the 5 traditional summary statistics. In my opinion, the most interesting result out of these 5 summary statistics is the mean for Heart Disease Death Counts which is 12823. Even though many of the states have much lower Heart Disease Death Counts than 12823, there are a few states, notably California and Texas that pull the average up!*

```{R - dplyr Summary Statistics Overall}
#2 functions using Group by categorical variable
Mortality_joined %>% group_by(STATE, YEAR) %>% summarize_at(c("Heart_Disease_Deaths", "Heart_Disease_Mortality_per_100000_People", "Hypertension_Deaths", "Hypertension_Mortality_per_100000_People"), mean)
  
Mortality_joined %>% group_by(YEAR) %>% summarize_at(c("Heart_Disease_Deaths", "Heart_Disease_Mortality_per_100000_People", "Hypertension_Deaths", "Hypertension_Mortality_per_100000_People"), sd)

#define my own function
Per_10000 <- function(x) {
  value <- x/10
  return(value)
}

Mortality_joined %>% group_by(STATE) %>% summarize(Heart_Disease_Mortality_per_10000_People = Per_10000(Heart_Disease_Mortality_per_100000_People))

#n() inside group_by
Mortality_joined %>% group_by(STATE) %>% summarize(Heart_Disease_and_Hypertension_Count = n())
Mortality_joined %>% group_by(YEAR) %>% summarize(Heart_Disease_and_Hypertension_Count = n())

#table using kable

Mortality_joined %>% group_by(YEAR) %>% summarize_at(c("Heart_Disease_Deaths", "Heart_Disease_Mortality_per_100000_People", "Hypertension_Deaths", "Hypertension_Mortality_per_100000_People"), sd) %>% kbl(caption = "Heart Disease and Hypertension Statistics by Year", col.names = c("Year", "Heart Disease Deaths", "Heart Disease Deaths per 100,000 People", "Hypertension Deaths", "Hypertension Deaths per 100,000 People"), align=rep('c')) %>% kable_material_dark()

#table using pivot_wider that can be used to make a table using kable package
Mortality_joined %>% pivot_wider(names_from = YEAR, values_from = Heart_Disease_Deaths)
```

*For the procedure for my summary statistics functions, I will be piping my joined dataset into all of the functions. First, I used the group_by function to group by either 1 or 2 categorical variables as shown above. Grouping by 2 categorical variables allowed me to group both years and states together so that I can see all the data for a state across the years that are available. Then, I defined my own function called Per_10000 and used it to compute the Heart Disease Mortality per 10,000 people and create a new column with it. Then, using n() inside group_by and summarize, I was able to count the number of years that data was available for each state and also the number of states that data was available for each year. Then, using the kable package, I was able to create a nice looking table to display the standard deviation for each of my columns in a dark theme with edited column titles for simplicity. Lastly, I showed that I could potentially use pivot wider and the kable package to create a wide table. However, at this moment, such a function is not super useful unless I want to untidy the data.*

#### Visualizing

```{R fig.height = 5, fig.width = 15}
library(ggplot2)
ggplot(Mortality_joined, aes(x = STATE, y = Heart_Disease_Deaths, fill = YEAR)) + 
  geom_bar(aes(x = STATE, y = Heart_Disease_Deaths, position = "fill"), stat = "identity") + 
  geom_smooth(aes(group = YEAR), method = "loess", se=F) +
  scale_y_continuous(breaks = seq(0,600000, by = 100000)) + 
  ggtitle("Heart Disease Death Counts by State and Year") + 
  ylab("Heart Disease Death Counts") + 
  xlab("State") +
  theme_light()
```

Your discussion of plot 1

```{R,fig.width = 8}
ggplot(Mortality_joined, aes(x = YEAR, y = Hypertension_Deaths)) + 
  geom_point(aes(color = STATE)) + 
  geom_smooth(method = "lm") +
  scale_x_discrete(limits = c(2005:2019)) +
  ggtitle("Hypertension Death Counts by Year") + 
  ylab("Hypertension Death Counts") + 
  xlab("Year") +
  theme_classic()
```

Your discussion of plot 2

```{R, fig.width = 11.5}
ggplot(Mortality_joined, aes(x = YEAR, y = Hypertension_Mortality_per_100000_People)) + 
  geom_point(aes(color = STATE)) +
  geom_line(aes(group = STATE, color = STATE), method = "loess", se=F) +
  scale_x_discrete(limits = c(2005:2019)) +
  ggtitle("Hypertension Mortality per 100,000 People by State and Year") + 
  ylab("Hypertension Mortality per 100,000 People") + 
  xlab("Year") +
  theme_linedraw()
```

Your discussion of plot 3

#### Concluding Remarks

This was very FUN!




